### Основы информационных систем

#### Надежность, масштабируемость, удобство

- **Надёжность** - как система реагирует на сбои
- Сбой - когда кусочек системы работает ненормально
- Отказ - когда вся система не работает

Чтобы система была устройства к сбоям, и чтобы это не переходило в отказ, нужно постоянно тестировать систему,
искусственно вызывая сбои (например что-то отключать, и проверять, что система работает)

Виды сбоев:

- Аппаратные сбои - когда железо хромает, помогает сделать несколько железяк (напр raid массивы)
- Программные сбои - ну понятно, всякие кейсы когда сторонние системы не робят, лечится допущениями и ретраями
- Человеческий фактор - тоже понятно, когда чето неправильно настроили, лечится откатами, песочницей, тестированием,
  метрики

- **Масштабируемость** - как система реагирует на возросшую нагрузки

Важно понимать что нагрузка возросла. Для этого используются _параметры нагрузки_

Примеры параметров нагрузки: запросы в секунду, соотношение операций записи и чтения

Способы чтения на примере тви:

- все твиты в однну табличку, затем для получения фида делаем джоин по подпискам
- другой подход делаем кеши с фидами и при публикации твита вставляем в каждый фид
- второй вар работал лучше потому что кол-во записей меньше кол-ва чтений
- Но когда подписчиков много, то это жестко - запись в лям фидов
- Так что можно использовать гибридный подход

Время ответа хорошо трекать с помощью перцентилей:

- 50 перцентиль - медиана
- Перцентили выше (95, 99) могут быть важны в случае больших запросов от клиентов (напр большая корзина)

Виды масштабирования:

- Масштабирование вертикальное - качаем одну машину
- Горизонтальное - качаем количество машин
- На практике можно комбинировать - несколько мощных тачек
- Авто масштабирование - когда на основе нагрузки поднимаются новые тачки
- Можно и в ручную делать, тоже норм варик, не будет неожиданностей (типа бессмысленного прироста тачек при ддос атаке)

Универсальной архитектуры не существует, всегда все зависит от параметров нагрузки, хотя есть паттерны масштабирования

**Удобство сопровождения** - когда мало Легаси; также включает в себя эксплуатацию, простоту и расширяемость

- _Эксплуатация_ - удобство работы системы для операторов
- Операторы оч важны, и хорошие операторы могут работать и с плохими системами
- В обязанности оператораров входят различные типовые операции, удобство эксплуатации заключается в автоматизации таких
  операций
- _Простота_ - ну понятно, если сложно, то сложно
- Чтобы уменьшить сложность, можно накидать слои абстракции (напр юзать высокоуровневые языки по сравнению с
  низкоуровневыми)

#### Модели данных и языки запросов

- SQL = отношения (таблицы) с кортежами (строками)
- NoSQL = Not Only SQL

Причины появления NoSQL:

- Новые возможности масштабирования
- Осс > клозед сурс
- Более сложные запросы
- Динамические схемы
- Объектно-реляционное несоответствие
    - Модели в коде могут иметь сложные связи, типа массива
    - В случае sql нужно делать хуилиард таблиц, и джоинить
    - А в случае NoSQL можно выразить все одним jsonом
    - Хотя и в sql можно json-поля сделать

##### Связи «многие-к-одному» и «многие-ко-многим»-

Можно хранить повторяющиеся данные (напр. должность) текстом, можно ФК на табличку использовать:

- Если текст, то будет жёсткое дублирование, и при замене может возникнуть ситуация когда что-то обновилось, а что-то
  нет
- Использование фк удобнее, потому что единообразие, удобно редачить, поиск удобный, локализацию можно сделать
- Использование фк вместо текста называется _нормализацией_
- Соответственно _денормализация_ - наоборот когда используется текст вместо фк

Поначалу может показаться что делать строки это просто и удобно, но потом текст может разразрастись до сущности (
название компании > сущность компания) - так появляется связь многие ко многим

Ни реляционная модель, ни документоориентированная модель не может быть универсальной в контексте простоты кода
приложения. Так документоориентированная модель сосет когда в приложении много связей, приходится делать несколько
запросов к бд и склеивать данные, что усложняет код

##### Динамическая и строгая схемы данных

- Динамическая схема = Неструктурированная модель в документоориентированной бд = schema less = _schema on read_ - то
  есть
  проверка типов происходит в коде; это удобно когда данные могут быть разных типов (кейс сторонних систем)
- Строгая схема = Реляционный подход = _schema on write_ - когда мы записываем данные и они будут соответствовать схеме
  данных; и при чтении необязательно проверять типы, они также будут соответствовать схеме
- Динамическую схему очевидно проще менять: так если хотим разбить имя на имя и фамилию, достаточно написать код; в то
  время как для строгой схемы нужно писать миграцию.
- В некоторых СУБД (мускуль) миграция может вызвать простой системы,
  тк
  применение миграции занимает много времени, обойти это можно используя утилиты или в случае с update (которая может
  тормозить в любой бд) можно выставить значение по умолчанию и заполнять в коде
- _Локальность данных_ = все в одном месте (не разнесено по нескольким таблицам) хороша когда используются все данные (
  кейс
  веб странички); но при этом документ выгружается целиком, что может быть не экономно. Аналогично при записи документ
  перезаписывается целиком, что тоже может занимать больше времени по сравнению с реляционной бд; так что лучше когда
  документы мелкие

##### Декларативный и императивный подходы

- SQL - декларативный подход к запросу данных, то есть мы пишем лишь то, что нам нужно, используя ограниченный
  синтаксис,
  а как данные будут получены решает оптимизатор. Также такой подход облегчает внутренние оптимизации и параллелизм.
- Императивный подход напротив требует написания кода: например получаем все доки, итерируемся по ним в цикле, фильтруя
  данные
- Ещё пример декларативного подхода - css по сравнению с js. Поиск css-селектора осуществляется браузером, так что
  создатели браузера могут оптимизировать его работу и не нужно будет переписывать код, js-код по поиску элемента
  оптимизируешь только ты, и придется менять код
- MapReduce - гибридный подход - пишешь чистые функции, которые могут выполняться в любом порядке и параллельно
    - Минус - в отсутствии декларативности
    - но в монге есть aggregation pipeline синтаксис, который позволяет писать
      мап-редьюс с помощью ограниченного синтаксиса, что обеспечивает внутреннюю оптимизацию

##### Графовые модели данных

- Графовые модели данных - соцсети, ссылки на веб страницах, дороги
- Граф состоит из вершин и ребер
- Алгоритмы на графах поиск кратчайшего пути для автомобилей, pageRank для рейтинга веб страниц

**Графы свойств**

- Графы свойств - neo4j, titan, infinite graph
- Вершины содержат айди, входящие и исходящие ребра и произвольные свойства
- Ребра содержат айди, начальные и конечные вершины и произвольные свойства
- И на основе этих данных можно соединять ребрами любые вершины, совершать обход графа, и хранить разную информации

Графы хороши тем что их можно расширять:

- например у нас есть два человека (2 вершины), два города, в которых они
  родились (2 вершины, но уже другая сущность), и два аллергена, на которые у них аллергия (опять 2 вершины с другим
  смыслом),
- таким образом можно получить у каких людей аллергия на определенный продукт или по стране получить ее
  граждан (то есть довольно гибко)

Cypher - язык запросов для графов (neo4j), там все стрелочками отписывается, плюс он декларативный, что хорошо

Все это можно реализовать как 2 таблицы - таблица вершин и таблица ребер, и использовать обычный SQL, но запрос будет
больше и сложнее для понимания, так что своей задаче - свой инструмент

**Тройные кортежи = Turtle формат = N3 (notation 3)**

- Субъект, предикат, объект
- Если объект - примитив (число, строка), то предикат и объект - пара ключ-значение (Маша, возраст, 20 => возраст=20)
- Если объект - вершина, то предикат - ребро (Маша, жената на, Пете => Маша и Петя - вершины, жената на - ребро)
- Семантическая паутина = RDF - альтернативный способ записи тройных кортежей для веб страниц в xml-формате
- SPARQL - язык запросов для тройных кортежей
- Datalog - предшественник Cypher и SPARQL; сам язык - подмножество Prolog - так что это логическое программирование;
- Cascalog - Hadoop реализация языка

#### Подсистемы хранения и извлечения данных

Структуры данных SQL = подсистемы хранения = Log-structured и page-oriented

**Log-structured подход и Индексы**

- Log-structured - данные записываются в конец файла (журнала, log)
- Индекс - структура данных для поиска, производная от данных
- Индексы можно добавлять и удалять, не затрагивая содержимое бд
- Поддержка индексов требует затрат на запись. Любая запись в бд будет сопровождаться обновлением индекса
- Так что индексы ускоряют чтение, но замедляют запись, и создание индексов лежит на плечах у разраба

**Хеш-индекс**

- Допустим бд - файл со строками ключ-значение, тогда можно сделать хеш-таблицу, где ключ - это ключ строки бд, а
  значение - номер строки, тогда при поиске значения по ключу, вместо полного перебора строк файла, мы получим из
  хеш-таблицы номер строки и там будет искомое значение ключа
- Хеш-индекс используется в системе хранения Bitcask в NoSQL бд Riak, и такую систему хорошо использовать для счётчиков
  просмотров

- При log-structured подходе есть проблема того, что файл с логом будет большим, решается она разбивкой файла на
несколько
файлов лимитированного размера (сегменты) + применение уплотнения (compaction) - т.е. отбрасывание дублирующихся
ключей
- Затем получившиеся стройные файлы можно слить
- Таким образом получаем файлы-сегменты и по ним строим хеш-таблицы
и храним в ореративе

Типовые проблемы и решения при таком подходе:

- Формат файла - бинарный
- Удаление - делаем спец метку (tombstone), которая говорит о том что нужно игнорировать предыдущие значения ключа
- Сбои - чтобы не считать заново хеш-таблицы, можно хранить их на диске
- Недописанное записи - используем контрольные суммы
- Конкурентный доступ - несколько файлов и несколько сегментов - значит можно параллелить чтение
- Почему запись в конец файла? Потому что быстрые операции, лёгкость конкурентного доступа и восстановления, решение
  проблемы фрагментирования
- Ограничения хеш-таблиц: они должны помещаться в оперативу, и запросы по диапазону неэффективны

##### Ss-таблицы и lsm-деревья

- Ss-таблица (sorted string table) - формат файла, где ключи отсортированы + ключ встречается в сегменте один раз
- Формирование ss-таблицы получается путем эффективного слияния нескольких сегментов в блок
- Индекс при этом получается разряженным (не обязан хранить все ключи), и можно искать ключ используя факт что он
  находится между двумя другими ключами
- Сами блоки небольшого размера, по ним быстро происходит чтение + можно сжать
- Как происходит вставка с сохранением порядка? Используются деревья (красно-черные, avl) - memtable - которые
  размещаются
  в оперативе
- Когда размер memtable достигает лимита, то сохраняем ss-table на диск
- Также ведём журнал с записью в конец для кейса восстановления
- Этот алгоритм используется в подсистемах хранения LevelDb и RocksDb, которые используются в Riak. Также аналогичная
  подсистема в Cassandra и HBase.
- Вся эта система (с уплотнением, слиянием отсортированных файлов) называется LSM  (Log-Structured Merge-Tree)
- Также похожий принцип используется в Lucene - поисковый движок, на котором работает Elastic Search. Ключ-значение
  состоит из терма и списка айди документов, которые содержат терм
- У такой системы есть проблема: поиск ключей, которых нет, может занимать много времени - но для этого есть спец
  структуры данных, типа Фильтр Блума
- Уплотнение и слияние бывает разным: size-tiered compaction, leveled compaction

##### B-деревья

- Хранят пары Ключ-значение, ключи отсортированы
- В отличие от LSM, данные разбиваются не на сегменты переменного размера, а на блоки/страницы фиксированного размера,
  аналогично разбивке на диске
- Каждый блок имеет ссылку на другой блок, так можно создать дерево страниц, по которому просто искать
- Одна из страниц - корневая, представляет из себя ключи и ссылки на дочерние страницы; ключи указывают диапазон страниц
- То есть
  `10 (граничный ключ) ссылка (на диапазон страниц в границах ключей) 20 (второй граничный ключ)`
- При поиске постепенно сужаются границы ключей, и мы попадаем в страницу-лист, где ключу соответствует значение
- Branching factor - кол-во ссылок на дочерние страницы, зависит от дискового пространства
- Запись значения происходит как и поиск - O(log n). Если место кончилось, то блок разбивается на два, и родитель
  обновляется
- Отказоустойчивость достигается с помощью WAL (write ahead log) = redo log - куда записывается операция записи в
  b-дерево
  до совершения операции
- Конкурентная запись происходит с помощью latch - облегченный вариант блокировок

Усовершенствования b-деревьев:

- вместо перезаписи использовать копирование страницы, так улучшится восстановление и конкурентная запись
- граничные ключи можно сжимать, чтобы в блок попало больше ключей и уменьшилось количество уровней
- дополнительные указатели на блоки на одном уровне - ускорение поиска нескольких ключей без надобности возвращаться к
  родителям
- фрактальные деревья - микс с lsm-деревьями

Сравнение b-tree и lsm-tree:

- Чтение: B-tree > LSM-tree
- Запись: B-tree< Lsm-tree
- Размер на диске: B-tree< Lsm-tree

##### Минусы LSM

- Уплотнение может занимать время, и время на больших перцентилях возрастёт
- Большое количество операций на запись отсравивает уплотнение, что замедляет чтение

##### Еще про индексы

- Вторичные индексы - индексы по столбцам, не являющихся первичными ключами
- Значение ключа в индексе может быть строкой (напр айди записи), может быть ссылкой на кучу - heap file, где хранится
  строка. Использование кучт хорошо чтобы избавиться от дубликатов
- Ходить в кучу бывает не оч эффективно, тогда хранят строку, то есть индекс кластерный
- Covering index = index with included columns - использование обоих подходов.
- Когда запрос можно выполнить используя только данные из индекса - значит индекс охватывает запрос
- Concatenated index - когда ключ состоит из нескольких полей (фамилия и имя), при этом важен порядок! (Поиск по имени
  бесполезен)
- Многомерные индексы - когда ищем по области (поиск ресторанов в промежутке координат) - R-дерево
- Fuzzy-запросы - когда ищем текст с ошибками. Помогает использование редакторского расстояния.
- In-memory db - когда храним все в оперативе, примеры: redis, memcache. Для надёжности можно делать слепки на диск,
  использовать спец железо, журналирование и тд. Если место кончается, то можно использовать антикеширование - старье на
  диск складывать

##### OLAP

- Oltp - обработка транзакций в реальном времени - чтение небольшого количества записей, пользователь интерактивно
  добавляет/меняет записи, скорость выполнения транзакций должна быть высокой
- Olap - аналитическая обработка данных в реальном времени - всякая статистика - обработка большого количества записей ,
  причем лишь некоторых столбцов; запросы могут выполняться долго
- Data warehouse - склад данных - бд для аналитики
- ETL - extract - transform - load - процесс загрузки данных из oltp баз в data warehouse
- Примеры data warehouse: Teradata, Vertica, SAP HANA, ParAccel (Amazon Redshift), Hive, Spark, Big Query.
- Схема Звезда - типовая модель данных в складах данных. Ключевая сущность - таблица фактов - таблица из событий,
  которые состоят из столбцов (зачастую фк) других таблиц - таблиц изменений
- Схема Снежинка - подвид звезды, но таблицы изменений разбиваются на подизмерения
- В складах данных данные располагаются не построчно, а по столбцам - так в запросах, которые аггрегируют несколько
  полей, не придется грузить строки, в которых столбцов может быть сильно больше чем в запросе
- Также для оптимизации столбцы можно сжать, тк значения в них повторяется. Типовой метод сжатия - bitmap encoding -
  значения перебираются, и результат кодирование это количество нулей (значение отсутствует) и количество единиц (подряд
  идущие позиции:
  1 2 3 3 =>
  1 - 0 1
  2 - 1 1
  3 - 2 2
- Если нулей много, то битмапа разряжена, и ее можно дополнительно сжать алгоритмом кодирования длин серий
- Модели данных в Cassandra, HBase, BigTable используют понятие column family - это не то же самое что столбчатое
  хранилище
- На уровне железа для хранилищ данных применяется векторизированная обработка
- Сортировка строк по определённым столбцам имеет смысл для более эффективного поиска, и более эффективного сжатия
- Также можно хранить несколько сортировок (используется в Vertica)
- Материализованные сводные показатели - речь об агрегируемых функциях типа sum, и о том почему бы их не кешировать.
- Один из способов - materialized view - похоже на обычный view в бд (сокращенная форма запроса), но это именно
  результат выполнения запроса
- Data Cube = OLAP Cube - частный случай materialized view - сетка показателей по данным столбцов
- Важно понимать что materialized view полезны лишь для некоторых запросов

#### Кодирование и эволюция

- Rolling update= staged rollout - постепенный деплой - когда сервер развернут на нескольких инстанцах, обновление
  происходит не сразу а постепенно, по инстансу за раз, проверяя все ли ок
- При обновлении приложения - тут сам клиент решает обновляться ли ему

Совместимость:

- Обратная совместимость - новый код способен читать старые данные
- Прямая совместимость - старый код может читать новые данные

##### Сериализация

Данные приложения могут быть в двух состояниях: в виде объектов и структур данных в оперативе, либо в виде
последовательности байтов (Напр. Json) при передаче данных по сети или хранении на диске

Процесс перевода данных из формата оперативы в формат для передачи называется encoding=serialization=marshalling, а
обратный процесс - deciding=deserialization=parsing=demarshaling

Языки программирования предоставляют свой способ сериализации данных, напр pickle в Python, но это работает только в
рамках одного языка, не очень безопасно, контроль версий хромает, и производительность тоже
может хромать

Json, xml, csv - популярные, человеко-читаемые текстовые форматы данных. Но у них есть некоторые проблемы: отсутствие
возможности указания точности чисел и больших чисел, отсутствие
возможности
передавать двоичные данные (но обходится с помощью base64 кодирования, но и увеличивает размер данных), в коже
приходится зашивать логику парсинга, и csv довольно хрупкий формат

##### Бинарные форматы

Двоичное кодирование имеет смысл когда данных много

Для json и xml есть свои бинарные аналоги, типа messagepack, bson, wbxml, хотя они не сильно экономят место

Thrift и Protocol Buffers - либы двоичного кодирования, обе работают на основе схемы (на своем языке), по этой схеме
можно генерировать код. Суть кодирования в применении тегов полей - вместо хранения названия поля, просто храним цифру.
Совместимость обеспечивается за счёт того что новые добавляемые поля - необязательны

Avro - ещё один двоичный формат, применяемый в Hadoop, там все завязано на порядке полей. Есть два типа схем, схема
чтения (та что в коде), схема записи (мб в файле с данными, в бд, при согласовании при отправке по сети), и они могут не
совпадать, таким образом можно осуществлять эволюцию схемы. Ещё плюсом Avro является возможность генерировать схему на
лету, например из схемы бд, и забекапить бд в двоичном формате

Кодогенарция для бинарных форматов актуальна только для статически типизированных языков, также есть специальный язык
обработки данных Apache Pig

##### Передача данных

Данные передаются на разных уровнях: на уровне бд, на уровне сети, при асинхронном общении

При передаче данных на уровне бд может быть ситуация, когда схема обновилась, а часть инстанцев ещё нет, тут важно
обеспечить прямую совместимость и чтобы если новый инстанц записал данные в новое поле, важно чтобы старый код не стёр
эти новые данные при записи

И ещё раз нужно быть аккуратнее с миграцией данных, когда старые данные обновляются на новый лад, это тяжёлая операция

Сервис - то что предоставляет апи

Сервис-ориентироаанная арх= микросервиснач арх - когда у сервиса своя задача, напр сервер работает с бд

Веб-сервис - сервис использующий хттп

Rest и soap - 2 подхода к написанию веб сервисов

Rest(ful) подход: применение урла для доступа к ресурсам, активное использование возможностей хттп: аутентификация,
кешироаание, часто используется json, с помощью которого можно генерить доку и код (openapi, swagger)

Soap - xml подход, не использует возможности хттп, а свои спеки, описываемые с помощью wsdl. Этот подход сильно
завязывается на спец утилитах, так что где-то интеграцию с соап сделать сложно 

---

Rpc - обращение к сервису как будто это вызов кода
Проблемы:
- сеть: подключение может оборваться, может произойти дедлайн, или код выполниться, но ответ не получим (лечится дедупликацей=иденпотентность), время выполнения становится непредсказуемым 
- сложные типы данных сложно копировать, типы данных в разных языках могут быть разными 
Эти проблемы пытаются решить новые фреймворки - grpc, rest.li, finagle 
- используя описанные выше бинарные форматы данных 
- разграничивая локальный и внешний вызов, используя фьючи/промисы/стримы; которые также позволяют параллелить запросы 
- service discovery - поиск сервисов на айпи 
Но rest все равно пизже, тк большая поддержка и больше возможностей - экосистема крч. Rpc актуален лишь при взаимодействии между внутренними сервисами 
Совместимость в сетевом взаимодействии:
- rpc: как в двоичных форматах 
- soap: на основе xml схемы
- rest: новые необязательные параметры в json 
- также помогает версионирование апи 
Asynchronous message passing system - когда передаем данные от процесса к процессу, но используя очередь сообщений (message broker, message queue)
Плюсы очереди сообщений:
- надёжность: сообщения могут лежать в очереди, если сервис занят 
- ретраи 
- не нужно знать IP получателя 
- можно отправлять сообщение нескольким получателям
- отправитель только отправляет, и не ждёт ответа 
Примеры: rabbit mq, Kafka, redis 
Принцип работы: один процесс отправляет сообщение в очередь/топик, брокер доставляет сообщение потребителям / подписчикам 
Акторная модель - concurrency в рамках одного процесса, взаимодействие происходит с помощью асинхронных сообщений, гарантии доставки нет, каждый актор обрабатывает только одно сообщение (так что не надо париться о блокировках). Ну и можно на нескольких нодах запускат. Примеры: Akka, Orleans, erlang otp

Распределенные данные 
Зачем распределять бд / код на несколько машин:
- масштабирование - мощностей одной машины может не хватить, так что юзаем несколько 
- надёжность - одна тачка может упасть, а так будут запасные тачки 
- задержка - возникает если пользователи из разных регионов, так что имеет смысл разместить несколько тачек по разным регионам 
Но распределенность имеет свои нюансы, надо уметь готовить, ведь это усложнение системы/кода 
Репликация - когда данные копируются на другую бд (другой узел)
Партиционирование = секционирование = шардинг - когда данные размазаны по нескольким узлам 
Можно совмещать: для каждого шарда сделать реплики 
Репликация 
Просто скопировать статичные данные не сложно, сложно копировать данные которые меняются 
Алгоритмы репликации: single leader, multi leader, leadless
Ведущие и ведомые узлы (master/slave)
Как убедиться в том что данные во всех метриках?
Leader (master, primary) based replication
- Данные (запросы на запись) отправляются в leader, там они пишутся в локальное хранилище
- Мастер отправляет слейвам изменения - replication log = change stream, слейв их применяет к своей локальной копии бд 
- Клиент может читать из любого узла 
Синхронная и асинхронная репликация 
Ну понятно, синхронно - когда ждём записи в реплику, асинхронно - не ждем 
Можно миксовать: в какие-то слейвы синхронно, в какие-то асинхронно 
Плюсы синхронного в надёжности - гарантированно данные, попавшие на мастер, будут в слейве; но минус в том, что если слейв бкрахлит, то и мастер будет тормозить 
Асинхронные наоборот - быстрая запись в мастер, но нет гарантии 
Полу-синхронная конфигурация - когда один узел синхронный, а остальные асинхронные, и если синхронный узел тормозит, то один из асинхронных становится синхронным 
Ещё есть цепная репликация, используется в Ажуре 
Как создать ещё один слейв?
- на мастере делаем согласованный снимок бд на время
- копируем его на слейв 
- слейв запрашивает у мастера изменения с момента снимка; изменение, которое соотносится со снимком называется log sequence number в постгре binlog coordinates в мускуле 
- когда все сделано, говорят что слейв наверстал упущенное))
Как обеспечить надёжность при отключении любого узла? 
- если отключился слейв, то он может запросить изменения с момента отключения
- если отключился мастер, то делаем одного из слейвов мастером, и перенаправляем поток данных - процесс называется failover 
Можно это автоматизировать:
- сначала определяем что мастер отказал - точно установить отказ сложно, так что можно просто настроить дедлайн 
- затем выбираем слейва, который будет мастером; выбираем с самой свежей репликой или заранее предопределенный узел - controller node
- настроить систему, чтобы она начала писать в новый мастер (и перенаправлять слейвам) + старый мастер должен "осознать" что он теперь слейв 
Но у такого подхода тоже есть проблемы: 
- асинхронная операции могут не дойти до мастера, а мастер уже сменился - дичь крч - тут мало че можно поделать, и как вариант можно просто отбросить такое 
- но отбрасывание опасно когда идентификаторв находятся в других системах, напр. в кеше 
- может возникнуть ситуация когда два узла считают себя мастерами и тогда будут конфликты в данных
- ещё непонятно какой дедлайн ставить, ведь большой дедлайн приведет к простою, а маленький к лишним телодвижениям
Так что как вариант можно делать это вручную 
Что происходит внутри мастера?
Операторная репликация
Каждая операция (insert, delete, update) записывается в журнал, журнал отправляется слейву, слейв выполняет операции 
Проблемы подхода:
- функции типа now, random - будут генерировать иные данные 
- аналогично инкрементация 
- аналогично функции 
Это конечно решаемо, заменяя результаты функций фиксированным значением
WAL 
Используем тот же лог что и используется в b-tree
Но это темная связь с подсистемой данных, и важно чтобы все узлы имели одинаковую версию
Logical log 
Храним отдельный журнал для репликации 
Храним упрощённую запись операций, типа айди и какое поле на какое поле поменялось
Такой подход также называется change data capture 
Триггерная репликация 
Репликация с помощью кода/ утилит
Oracle Golden Gate, Databus, Bucardo 
Плюс такой репликации в гибкости
Проблемы задержки репликации 
Read-scaling архитектура - когда достаточно одного мастера для записи, и мы оптимизируем чтение из разных узлов, - может быть привлекательна, тк много чтений, мало записи 
Но это работает при асинхронной репликации, иначе при синхронной репликации при сборе одного из узлов, запись будет тоже сбоить 
И крч в случае асинхронной записи может быть ситуация когда данные не везде актуальны в определенный момент, и лишь через какое-то время (replication lag) будут согласованы на всех узлах - это называется конечная согласованность = eventual consistentcy
Такая ситуация влечет за собой проблемы:
Например если юзер что записал, а потом решил прочесть, и чтение произошло из реплики, которая ещё не актуально, и выглядит это так что как будто данные не записались 
Тут нужно делать read-after-write consistency=read-your-writes:
- Свои данные читать с мастера, чужие из слейвов 
- читать со слейвов спустя какое время
Монотонное чтение
Похожий проблемный кейс: юзер читает данные с одной реплики, потом со второй, и кажется что данные были, а потом пропали 
Помогает монотонное чтение записей - то есть для одного и того же юзера читаем из одной и той же реплики (например по хешу)
Согласованное префиксное чтение
Проблема: репликация одной записи происходит позже чем репликация другой более поздней
Решение - Consistent prefix read - когда связанные записи пишем в одну секцию (через один мастер)
Задержка репликации 
Решается использованием транзакций, но в распределённой среде это сделать сложнее
Репликация с несколькими мастерами = master-master 
Все то же самое (нужно отправлять изменения всем слейвам), но писать можно в несколько мастеров 
Полезно когда несколько ЦОД 
Но есть и минус - конфликты, первичные ключи и тд 
Ещё полезно когда делаем офлайн доступ - но это как цод только на девайсе
Аналогично - кейс редактирования дока несколькими пользователями 
Как быть с конфликтами 
- Просто избегать их - например чтобы один юзер писал только в один цод 
- конвергентное разрешение конфликтов
  - определение победителя - запись с максимальной датой - last write wins 
  - приоритет мастера - у кого больше, того и тапки (данные)
  - смерджить конфликты 
  - записать конфликты и выводить их, чтобы юзер сам зарезолвил 
Алгоритмы автоматического разрешения конфликтов
- conflict-free replicated datatype, CRDT
- mergeable persistent data structu-res = git
- operational transformation - google docs 
Конфликты 
- Когда 2 чела меняют заголовок страницы в вики 
- Когда 2 чела бронируют одно и то же место в кино 
Replication Topology
Кольцо, звезда, каждый с каждым
Репликация без мастера 
При сбое одной из реплик, можно совершать операции сразу над несколькими репликами 
При восстановлении аналогично при чтении видим что старые данные и пишем новые в восстановленную реплику
Anti-entropy процесс - фоновый процесс выискиввющий несогласованности в репликах 
Кворум 
w+r>n - условие устойчивости 
w - успешные записи 
r - сколько реплик нужно прочитать чтобы получить актуальные данные 
n - кол во всех реплик 
Что такое конкурентность
Когда 2 записи не знают друг о друге 
Партиционирование 
Ака шардинг 
Когда разрезаем базу на несколько баз поменьше 
Репликация для партиционирования не сильно отличается 
Skewed - когда данные не равномерно размазаны по шардам 
Hot spot - шард с наибольшей нагрузкой
Способы шардирования
- рандом - равномерно пишем, но читать хз откуда, так что надо параллелить чтение 
- по ключу - каждый шард - диапазон ключей (BigTable, HBase, RethinkDb, Mongo); минус - хот споты по ключам, например если ключ - дата, то запись будет только в один шард 
- по хешу - применяем хеш-функцию по ключу (md5, Фаулер Нола Во) (Cassandra, Mongo, Voldemort); минус - отсутствие возможности получать значения в промежутке
Ещё минус хеширования (да и любого вида распределения нагрузки) - запросы по одному ключу (например знаменитости в соцсетях)
Решение - хардкод горячих ключей и добавление к ним суффикса (например рандомного числа) - так будет разбиение 
Секционирование по вторичным индексам 
- По документам: строим вторичные индексы в каждом шарде - local index - это удобно писать, но чтение требует запроса ко всем шардам - называется scatter/gather 
- По термам - global index - размазывает индекс по диапазонам (как ключи) / хешу - читать становится легче, но и писать бывает нужно в несколько индексов 
Rebalancing - перераспределение данных в следствие возросшей нагрузки или других причин. Важно чтобы при перебалснсировке все продолжало работать, а после нагрузка должна быть равномерно распределена 
Как сделать? Разбить секции на ещё более мелкие секции 
Service discovery - прога которая ищет сервисы: например добавили новый шард - и код должен о нем узнать от сервиса - Zookeeper 
Транзакции
Транзакция - выполнение нескольких операций как одной с фиксацией при успехе, и пребыванием и отказом в случае неудачи
Транзакции дают некоторые гарантии, но это не бесплатное удовольствие
ACID (atomicity, consistency, isolation, durability — атомарность, согласованность, изоляция и сохраняемость).
Атомарность - когда делаем несколько записей, то если на полпути что-то сломается, то все откатиться
Согласованность - когда данные подчиняются правилам (инварианты) (типа дебет с кредитом совпадает) но это скорее свойство приложения 
Изоляция = Сериализуемость - как будто каждая транзакция выполняется последовательно
Сохраняемость - чтоб данные не пропадали, гарантированно хранились
Сериализуемость - это дорого, накладно 
Так зачастую используются слабые уровни изоляции
Read committed - никаких грязных чтений, записей 
Грязное чтение - когда читает то что записали, но не зафиксировали/не откатили (в середине транзакции) 
Грязная запись - когда две транзакции пишут и происходит конфликт
Read committed фиксит это блокируя строки в случае чтения, и храня старое значение при чтении
Unrepeatable read = read skrew = асимметрия чтения - проблема read committed - когда читаем из двух источников, и между чтениями была транзакция
Это проблема в кейсе резервного копирования и аналитики
Изоляция снимков состояния помогает - когда транзакция работает только с согласованным снимком данных
multiversion concurrency control, MVCC - хранение нескольких снимков
Реализуется это выставлением айди транзакции каждому снимку, при очередной транзакции операции других транзакций игнорируются
Индексы при таком способе либо строятся для всех данных, либо делаются копии
Lost update - пример одновременное обновление счётчика
Решение:
- Атомарная запись - atomic write
- Явные блокировки строк - select for update 
- Автоматический поиск потеряшек 
- compare-and-set - когда обновляем то что было до записи 
Write skrew - ассисетрия записи - когда выполняем 2 записи параллельно в разные строки, чекая условия, и таким образом условие в обоих случаях выполняется, хотя при последовательной обработке второй раз условие бы не прошло 
Phantom - то же самое, но чтение после записи меняет селект - фиксится с помощью materializing conflicts и сериализуемость 
Виды сериализуемости:
- последовательное выполнение 
- Двухфазная блокировка (2PL)
- Сериализуемая изоляция снимков состояния (SSI)
Последовательное выполнение - хорошо работает в in-memory db. Также иногда хорошо работают stored procedures
Двухфазная блокировка (2PL) - если транзакция на чтение уже началась, то транзакции на запись ожидают ее фиксации и наоборот
Реализуется с помощью 2 видов блокировки: shared, exclusive
2 фазы: получение блокировок и установка
В случае deadlock одна из транзакции прерывается, так что приложению надо перезапустить ее
Предикатная блокировка - блокировка строк удовлетворяющих условиях - помогает в предотвращении фантомов 
Index-range lock, next-key locking - упрощение предикатной блокировки - блокировка происходит по соответственно индексу
Сериализуемая изоляция снимков состояния SSI - оптимистический метод управления конкурентным доступом - когда считаем что все транзакции происходят без проблем, а в случае проблем, они прерываются 
Как в кейсе изоляции снимков состояния определяется что данные, которая использует транзакция А, изменились транзакцией Б? Происходит проверка были ли зафиксированы транзакции Б при фиксации транзакции А + прерываем если была 
Проблемы распределенных систем
Partial failure - сбой одной тачки 
Сетевые сбои
Ну тут понятно - кинули запрос, а ответ хз
Способы лечения:
- тайм-ауты
- вывод ошибки юзеру 
Часики
Когда много тачек, время на каждой тачке может чуток отличаться, так что NTP можно юзать 
Часы истинного времени (time-of-day clock) - UTC timestamp, хорошо для времени событий, но могут быть прыжки во времени 
Монотонные часы (monotonic clock) - хороши для подсчёта длительности времени (дельта двух дат), но точное время по ним не стоит смотреть 
Логические часы - измеряют относительность событий
Система может работать, но заниматься не передачей данных по сети, а чем-то иным, например garbage collection 
Другие узлы должны коллективно принять решение что он "умер" - это кворум 
Согласованность и Консенсус 
Линеаризуемость = атомная/сильная/непосредственная/внешняя согласованность - иллюзия того что реплика одна - т.е. все клиенты видят одно и то же. Делается это созданием реестра, где при каждом новом чтении старые значения не возвращаются
Юзкейсы: блокировка ведущего узла Zookeeper, etcd; уникальность значений в бд (типа юзернейм)
Отсутствие линеаризуемости - когда один читает новое значение, а другой старое
Линеаризуемость уменьшает производительность, так что часто можно отказаться от нее
Все проблемы масштабирования сводятся к тому чтобы все упорядочить, выстроить последовательность событий, причинность
Соответственно есть причинная согласованность - более слабый вариант согласованности, где есть последовательность выполнения операций, но возможны конкурентные операции 
Временные метки Лампорта - айди ноды + счётчик + передача максимума счётчика - помогает достичь упорядоченности
Рассылка общей последовательности - когда данные не теряются и упорядоченная доставка - Zookeeper это делает
Проблема атомарной фиксации - когда транзакция выполняется на нескольких тачках, и если где-то фейл, то там где саксесс надо откатывать - пример консенсуса 
Two phase commit - алго распределенных транзакций - есть координатор который перед фиксацией опрашивает остальные узлы готовы ли они к фиксации, если все ответили да, то все фиксируется, иначе откат; происходит это с помощью айди транзакции и ретраев/ожидания в случае неудачи при фиксации
Консенсус - когда все узлы приняли одно решение + решение устойчиво 
Типичная реализация - репликация с одним узлом 
Но если ведущий узел отвалился, то вся система встанет 
Решение - нумерация периодов - ведущему узлу присваивается номер, если он не отвечает, то голосованием решается кто будет новым ведущим и ему присваивается новый номер, когда первый восстанавливается, он сравнивает свое значение с текущим, и если он больше, то он становится ведущим - это фиксит проблему нескольких интеллектов 






